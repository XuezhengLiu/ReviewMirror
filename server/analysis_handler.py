import json
import sys
sys.path.append("/mnt/access")
import numpy as np
import pandas as pd
from transformers import RobertaForSequenceClassification, RobertaTokenizer
import torch
import re
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from textblob import TextBlob
import io
from datetime import datetime


def predict(query, model, tokenizer, device="cpu"):
    tokens = tokenizer.encode(query)
    all_tokens = len(tokens)
    tokens = tokens[:tokenizer.model_max_length - 2]
    used_tokens = len(tokens)
    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)
    mask = torch.ones_like(tokens)

    with torch.no_grad():
        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]
        probs = logits.softmax(dim=-1)

    fake, real = probs.detach().cpu().flatten().numpy().tolist()
    return real


def handle_get(event, context):
    review = event['queryStringParameters']['review'] # get review content
    if event['queryStringParameters']['review'] and event['queryStringParameters']['review'].strip(): # empty string check
        model_name = "roberta-base"
        tokenizer = RobertaTokenizer.from_pretrained('/mnt/access/roberta-base-file')
        model = torch.load('/mnt/access/ft-roberta-amazonreviews.pt', map_location ='cpu')
        
        
        # result=[]
        query = review
        prediction = predict(query,model,tokenizer)
        if prediction<0.5: # CG: Computer-generated fake reviews
            analysis_result='Analysis result: This content might be generated by computer'
        else: # OR: Original reviews (presumably human created and authentic)
            analysis_result='Analysis result: This content might be written by human'
        print(analysis_result)
        result={'result':analysis_result} # 
        return {
            'statusCode': 200,
            'headers': {
                "Access-Control-Allow-Headers": "Content-Type, X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methond": "DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT",
                "Access_Control-Allow-Credentials": "true",
                "X-Requested_With": "*"
                
            },
            'body': json.dumps(result)

        }
    
    
    else:
        result={'result':'Empty input'}
        return {
        'statusCode': 200,
        'headers': {
            "Access-Control-Allow-Headers": "Content-Type, X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methond": "DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT",
            "Access_Control-Allow-Credentials": "true",
            "X-Requested_With": "*"
            
        },
        'body': json.dumps(result)
    }


def handle_put(event, context):
    
    return {
        'statusCode': 200,
        'headers': {
            "Access-Control-Allow-Headers": "Content-Type, X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methond": "DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT",
            "Access_Control-Allow-Credentials": "true",
            "X-Requested_With": "*"
        },
        'body': ''
    }


def handle_del(event, context):
     return {
        'statusCode': 200,
        'headers': {
            "Access-Control-Allow-Headers": "Content-Type, X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methond": "DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT",
            "Access_Control-Allow-Credentials": "true",
            "X-Requested_With": "*"
        },
        'body': ''
    }




def handle_post(event, context):
    nltk.data.path.append("/mnt/access/nltk_data")
    from nltk.stem.wordnet import WordNetLemmatizer
    from sklearn.feature_extraction.text import TfidfVectorizer
    import matplotlib.pyplot as plt
    import seaborn as sns
    from wordcloud import WordCloud
    import heapq
    from io import BytesIO
    from nltk.corpus import stopwords
    import base64
    import demjson

    sid = SentimentIntensityAnalyzer()
    
    event = event['body']
    print(event)
    # print(event['body'])
    df = pd.DataFrame()
    data = demjson.decode(event)
    request_keys = data.keys()
    for i in request_keys:
        convert_to_json = json.loads(data[i])
        original_rating = convert_to_json['summary']['rating']
        reviews_total = convert_to_json['summary']['reviewsTotal']
        asin = convert_to_json['asin']
        for j in list(convert_to_json['reviews'].keys()):
                reviews_id = convert_to_json['reviews'][j]['reviewId']
                reviews_title = convert_to_json['reviews'][j]['reviewTitle']
                reviews_body = convert_to_json['reviews'][j]['reviewBody']
                reviews_rating = convert_to_json['reviews'][j]['reviewRating']
                reviews_date_utc = convert_to_json['reviews'][j]['reviewUTC']
                reviews_profile_id = convert_to_json['reviews'][j]['reviewPID']
                single_series_combine = pd.Series([reviews_id, reviews_title, reviews_body, reviews_rating, reviews_date_utc, reviews_profile_id])
                df = df.append(single_series_combine,ignore_index=True)
    
    df.columns = ['reviews.id','reviews.title','reviews.body','reviews.rating','reviews.date.utc','reviews.profile.id']

    total_review_number = len(df)
    #drop rows containing NA values in mentioned three columns
    appended_data = df.dropna(subset=['reviews.id', 'reviews.title','reviews.body'])
    #convert date utc iso to just date format
    appended_data['reviews.date.utc'] = pd.to_datetime(appended_data['reviews.date.utc']).dt.date
    #reset the index 
    appended_data.reset_index(drop=True,inplace=True)
    #Calculating rating based on sample fetch reviews
    previousRatingTotal = appended_data['reviews.rating'].sum()
    previousSizeTotal = len(appended_data)
    previousStarRating = previousRatingTotal / previousSizeTotal
    previousStarRating = round(previousStarRating,2)# rating of product based on size of dataframe
    
    #code for labelling sentimental title label using valder sentimental package
    appended_data['reviews.title.score'] = appended_data['reviews.title'].apply(lambda revtitle: sid.polarity_scores(revtitle))
    
    #Getting compound core and labelling postive tag for value greater than equal to zero or else negative for values less than zero
    appended_data['compound.title.score'] = appended_data['reviews.title.score'].apply(lambda score_title: score_title['compound'])
    appended_data['reviews.title.label'] = appended_data['compound.title.score'].apply(lambda c: 'pos' if c>= 0 else 'neg')
    appended_data.drop('reviews.title.score',axis =1, inplace=True )
    appended_data.drop('compound.title.score',axis =1, inplace=True )
    
    #code for labelling sentimental body label using valder sentimental package
    appended_data['reviews.body.score'] = appended_data['reviews.body'].apply(lambda revbody: sid.polarity_scores(revbody))
    appended_data['compound.body.score'] = appended_data['reviews.body.score'].apply(lambda score_body: score_body['compound'])
    
    #Getting compound core and labelling postive tag for value greater than equal to zero or else negative for values less than zero
    appended_data['reviews.body.label'] = appended_data['compound.body.score'].apply(lambda c: 'pos' if c>= 0 else 'neg')
    #appended_data['reviews.body.label'] = appended_data['compound.body.score'].apply(lambda c: 'pos' if c>= 0.05 else ('neu' if(c>(-0.05) and c<(0.05)) else 'neg') )
    appended_data.drop('reviews.body.score',axis =1, inplace=True )
    appended_data.drop('compound.body.score',axis =1, inplace=True )
    
    # 1. Reviews that take a dual perspective. Different tone in the review's headline and body
    
    #declaring empty list
    remove_reviews = []
    
    
    for i in range(len(appended_data)):
        #iterate through the whole dataset
        
        # checking if the sentiment of the body and the headline are not same
        if(appended_data["reviews.title.label"][i] != appended_data["reviews.body.label"][i]):
            # checking if the sentiment of the body is posistive and rating given is less than equal to three.
            if ((appended_data["reviews.body.label"][i] == "pos") and (appended_data["reviews.rating"][i]<=3)):
                #Recheck sentimental score using textblob package for both title and body to classify fake reviews with posistive sentiments and poor star ratings.
                if ((TextBlob(appended_data["reviews.body"][i])).sentiment.polarity >= 0.05 and (TextBlob(appended_data["reviews.title"][i])).sentiment.polarity >= 0.05):
                    remove_reviews.append(appended_data["reviews.id"][i])
    #remove rows classfied as fake reviews from dataset                
    appended_data = appended_data[~appended_data['reviews.id'].isin(remove_reviews)]
    appended_data.reset_index(drop=True,inplace=True)
    
    # 2. Reviews that are posted in a flood by the same user are all positive or negative. 
    # On the same day, a user posts (>=3) reviews, all of which are either positive or negative.
    
    #User posting (>3) reviews on the same day with all the reviews are either positive or negative.
    reviewer_group = appended_data.sort_values("reviews.profile.id").groupby("reviews.profile.id")
    reviewer_group_list = appended_data["reviews.profile.id"].unique().tolist()
    for i in range(len(reviewer_group_list)):
        
        reviewer_reviews = reviewer_group.get_group(reviewer_group_list[i])
        # Dataframe of data of each cutomers
        
        dates_list = reviewer_reviews["reviews.date.utc"].unique().tolist()
        # list of dates of reviews written by each customers
        
        reviews_by_date = reviewer_reviews.groupby("reviews.date.utc")
        # gouping reviews by date for each cutomers
        
        for j in range(len(dates_list)):
            # iterating through all dates
            
            reviews_by_date_for_pos = []
            reviews_by_date_for_neg = []
    
            df = reviews_by_date.get_group(dates_list[j])
            #dataframe storing the details for each details for each customers
            
            indices = df.index.tolist()
            # indices of dataframe of each date
            
            for k in range(len(df)):
                if (df["reviews.body.label"][ indices[k] ] == 'neg'):
                    reviews_by_date_for_neg.append(df["reviews.id"][ indices[k] ])
                else:
                    reviews_by_date_for_pos.append(df["reviews.id"][ indices[k] ])
            
            if(len(reviews_by_date_for_pos)>=3):
                remove_reviews.extend(reviews_by_date_for_pos)
                
            if(len(reviews_by_date_for_neg)>=3):
                remove_reviews.extend(reviews_by_date_for_neg)
    
    appended_data = appended_data[~appended_data['reviews.id'].isin(remove_reviews)]
    appended_data.reset_index(drop=True,inplace=True)
    
    # 3. Reviews in which the Reviewer employs an intimidating tone in order to purchase the product (Verbs over nouns).
    for i in range(len(appended_data)):
        words = nltk.word_tokenize(str(appended_data["reviews.body"][i]))
        tagged_words = nltk.pos_tag(words)
        nouns_count = 0
        verbs_count = 0
        
        for j in range(len(tagged_words)):
            
            if (tagged_words[j][1].startswith("NN")):
                nouns_count +=1
                
            if (tagged_words[j][1].startswith("VB")):
                verbs_count +=1
                
        if(verbs_count > nouns_count):
            
            remove_reviews.append(appended_data["reviews.id"][i])
            
                
    appended_data = appended_data[~appended_data['reviews.id'].isin(remove_reviews)]
    appended_data.reset_index(drop=True,inplace=True)        
    
    # 4. Reviews in which the reviewer is narrating his or her own narrative 
    # ( Reviews that contain a high number of first-person pronouns).
    for i in range(len(appended_data)):
    
        words = nltk.word_tokenize(str(appended_data["reviews.body"][i].lower()))
        
        sentence = nltk.sent_tokenize(str(appended_data["reviews.body"][i].lower()))
        
        count = 0
        if(len(sentence)>4):
            
            for j in range(len(words)):
                
                if(words[j] == "i" or words[j] == "we"):
                    count+=1
                    
            if(count > len(sentence)/2):
                
                remove_reviews.append(appended_data["reviews.id"][i])
                
    appended_data = appended_data[~appended_data['reviews.id'].isin(remove_reviews)]
    appended_data.reset_index(drop=True,inplace=True)
    
    # new rating
    newRatingTotal = appended_data['reviews.rating'].sum()
    newSizeTotal = len(appended_data)
    newStarRating = round((newRatingTotal / newSizeTotal),2)
    
    review_reliability = int(100*(len(appended_data)/previousSizeTotal))
    
    # Applying text summarization on raw review text in order to extract insights through wordcloud.

    review_text ="" # a blank variable of str 
    for r in appended_data['reviews.body']: # looping through all reviews and joining them in string
        review_text += str(r)
    sentence_list = nltk.sent_tokenize(review_text) # getting the number of sentence
    
    #Finding weighted frequencies of occurrence
    stwords = nltk.corpus.stopwords.words('english')
    word_frequencies = {}  
    for word in nltk.word_tokenize(review_text):
        if word not in stwords:
            if word not in word_frequencies.keys():
                word_frequencies[word] = 1
            else:
                word_frequencies[word] += 1
    #divide the frequency of the word by the frequency of the most occurring word.
    maximum_frequncy = max(word_frequencies.values())
    for word in word_frequencies.keys():
        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
    #scores for each sentence is calculated by adding weighted frequencies for each word.
    sentence_scores = {} #sentence_scores dictionary consists of the sentences along with their scores
    for sent in sentence_list:
        for word in nltk.word_tokenize(sent.lower()):
            if word in word_frequencies.keys():
                if len(sent.split(' ')) < 30:
                    if sent not in sentence_scores.keys():
                        sentence_scores[sent] = word_frequencies[word]
                    else:
                        sentence_scores[sent] += word_frequencies[word]
    # getiing the summary of given reviews in half the number of sentence.
    n = int(round((len(sentence_list)/2),0))
    summary_sentences = heapq.nlargest(n, sentence_scores, key=sentence_scores.get)
    summary = ' '.join(summary_sentences)
    #Change to lower case and removing unwanted symbols incase if exists
    summary = re.sub("[^A-Za-z" "]+"," ",summary).lower()
    summary = re.sub("[0-9" "]+"," ",summary)
    
    # removes special characters with ' '
    summary = re.sub('[^a-zA-z\s]', '', summary)
    summary = re.sub('_', '', summary)
    
    # Change any white space to one space
    summary = re.sub('\s+', ' ', summary)
    
    # words that contained in sony speaker reviews
    summary_words = summary.split(" ")
    # Lemmatizing
    wordnet = WordNetLemmatizer()
    summary_words=[wordnet.lemmatize(word) for word in summary_words]
    # Filtering Stop Words
    stop_words = set(stopwords.words("english"))
    stop_words.update(['amazon','product'])
    summary_words = [w for w in summary_words if not w.casefold() in stop_words]
    # TFIDF: bigram
    bigrams_list = list(nltk.bigrams(summary_words))
    bigram = [' '.join(tup) for tup in bigrams_list]
    vectorizer = TfidfVectorizer(input=bigram,use_idf=True,ngram_range=(2,2))
    X = vectorizer.fit_transform(bigram)
    vectorizer.vocabulary_
    sum_words = X.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)
    #Genrating word Cloud
    words_dict = dict(words_freq)
    wordCloud = WordCloud(width=1500,height=1000,background_color='white',max_words=35)
    wordCloud.generate_from_frequencies(words_dict)
    plt.title('Most Frequently Occurring Bigrams')
    plt.imshow(wordCloud, interpolation='bilinear')
    plt.axis("off")
    
    figfile = BytesIO()
    plt.savefig(figfile, format='jpg')
    figfile.seek(0)  # rewind to beginning of file
    
    # convert bytes into base64
    figdataMFO_jpg = base64.b64encode(figfile.getvalue())
    
    # Genrating worcloud based on words sentiment.
    pos_word_list=[] # list for posistive words
    neg_word_list=[] # list for negative words
    # looping through all words and checking its polarity through nltk valder
    for word in words_freq:
        if (sid.polarity_scores(word[0])['compound']) >= 0.05:
            pos_word_list.append(word)
        elif (sid.polarity_scores(word[0])['compound']) <= -0.05:
            neg_word_list.append(word)
        else:
            pass
    #defining color function to color words depending on sentiment
    word_to_color = dict() # empty dictionary
    #looping through positive words list
    for word in pos_word_list:
        word_to_color[word[0]] = '#6BCB77' # green
    #looping through negative word list    
    for word in neg_word_list:
        word_to_color[word[0]] = '#FF6B6B' # red
        
    #color function to return color according to sentiment of words
    def color_func(word, *args, **kwargs):
        try:
            color = word_to_color[word]
        except KeyError:
            color =  '#000000' # black
        return color
    
    df_pos_neg = pd.DataFrame()
    for i in pos_word_list:
        group = 'pos'
        bigram = i[0]
        frequency = i[1]
        single_series_combine = pd.Series([group,bigram,frequency])
        df_pos_neg = df_pos_neg.append(single_series_combine,ignore_index=True)
    for i in neg_word_list:
        group = 'neg'
        bigram = i[0]
        frequency = i[1]
        single_series_combine = pd.Series([group,bigram,frequency])
        df_pos_neg = df_pos_neg.append(single_series_combine,ignore_index=True)
    # construct a Dataframe that contains group, bigram and frequency
    df_pos_neg.columns = ['group','bigram','frequency']
    # draw Most Frequently Occurring positive and negative words
    def pos_neg_show(combi_words_dict):
        wordCloud = WordCloud(width=1500,height=1000,background_color='white',collocations=False,color_func=color_func,max_font_size=170,relative_scaling=1,
                     min_font_size = 30, max_words = 35)
        wordCloud.generate_from_frequencies(combi_words_dict)
        plt.title('Most Frequently Occurring positive and negative bigrams in reviews. ')
        plt.imshow(wordCloud, interpolation='bilinear')
        plt.axis("off")
        figfile = BytesIO()
        plt.savefig(figfile, format='jpg')
        figfile.seek(0)  # rewind to beginning of file
        # convert bytes into base64
        figdataPosNeg_jpg = base64.b64encode(figfile.getvalue())
        return figdataPosNeg_jpg
        
    if len(df) < 36: # if length of dataframe less than max words 35, directly show the image
        combi_word_list = pos_word_list + neg_word_list # combine pos_word_list and neg_word_list
        combi_words_dict = dict(combi_word_list)
        figdataPosNeg_jpg = pos_neg_show(combi_words_dict)
    else: # length greater than 35
        if pos_word_list.__len__() > neg_word_list.__len__(): # positive word list is longer
            # sort the dataframe based on words frequency, if the frequency of several words is same then sort neg words firstly 
            # (ensure negative words still can be displayed )
            df_pos_neg.sort_values(['frequency', 'group'], ascending = [False, True],inplace=True)
            df_pos_neg = df_pos_neg.head(35)
            bigram = df_pos_neg['bigram']
            frequency = df_pos_neg['frequency']
            combi_words_dict = dict(zip(bigram,frequency))
            figdataPosNeg_jpg = pos_neg_show(combi_words_dict)      
        else:
            # sort the dataframe based on words frequency, if the frequency of several words is same then sort pos words firstly 
            # (ensure positive words still can be displayed )
            df_pos_neg.sort_values(['frequency', 'group'], ascending = [False, False],inplace=True)
            df_pos_neg = df_pos_neg.head(35)
            bigram = df_pos_neg['bigram']
            frequency = df_pos_neg['frequency']
            combi_words_dict = dict(zip(bigram,frequency))
            figdataPosNeg_jpg = pos_neg_show(combi_words_dict)
    
    result={'previousRating':previousStarRating,'newStarRating':newStarRating,'review_reliability':review_reliability,'MFO':(str(figdataMFO_jpg)[2:-1]),'PosNeg':str(figdataPosNeg_jpg)[2:-1]}
    print(str(result))

    
    return {
        'statusCode': 200,
        'headers': {
            "Access-Control-Allow-Headers": "Content-Type, X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methond": "DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT",
            "Access_Control-Allow-Credentials": "true",
            "X-Requested_With": "*"
            
        },
        'body': json.dumps(result)
        
    }


def lambda_handler(event, context):
    if event['httpMethod'] == 'GET':
        return handle_get(event, context)
    elif event['httpMethod'] == 'PUT':
        return handle_put(event, context)
    elif event['httpMethod'] == 'DELETE':
        return handle_del(event, context)
    elif event['httpMethod'] == 'POST':
        return handle_post(event, context)
    else:
        return {
            'statusCode': 200,
            'body': json.dumps('Hello from Lambda!')
        }
    
    
